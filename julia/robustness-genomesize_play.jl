using Random
using StatsBase
using Distributions
using Optim
using LinearAlgebra
using DifferentialEquations
using Plots
using StatsPlots

### TRANSFORMATION ###

"""
Convert a vector of edges (Cartesian indices) to a binary incidence matrix.
"""
function edges_to_matrix(n_rows, n_cols, edges)
    X = zeros(Bool, n_rows, n_cols)
    X[edges] .= true
    X
end

function matrix_to_edges(X)
    findall(X)
end



function vector_to_matrix_with_edges(n_rows, n_cols, v, edges)
    X = zeros(n_rows, n_cols)
    X[edges] = v
    X
end

function nested_vector_to_matrix(vv)
    m = zeros((length(vv), length(vv[1])))
    for i in 1:length(vv)
        m[i,:] = vv[i]
    end
    m
end

### INITIALIZATION ###

"""
Generate a set of receptor-effector edges corresponding to a binary matrix
generated by `generate_matrix_with_max_rowsum`.
"""
function generate_edges_with_max_receptor_degree(
    n_receptors::Int, n_effectors::Int, max_receptor_degree
)
    matrix_to_edges(generate_matrix_with_max_rowsum(
        n_receptors, n_effectors, max_receptor_degree
    ))
end

"""
Return a binary matrix whose row sums are drawn uniformly randomly from
`1:max_rowsum` and whose set of `true` entries in each row is sampled uniformly
from all sets with the drawn row sum.
"""
function generate_matrix_with_max_rowsum(
    n_rows::Int, n_cols::Int, max_rowsum::Int
)
    @assert n_rows >= 1
    @assert n_cols >= 1
    @assert max_rowsum >= 1

    X = zeros(Bool, (n_rows, n_cols))
    for i in 1:n_rows
        rowsum = rand(1:max_rowsum)
        X[i, sample(1:n_cols, rowsum, replace = false)] .= true
    end
    X
end

function generate_matrix_with_max_rowsum_redundant(
    n_rows::Int, n_cols::Int, max_rowsum::Int
)
    @assert n_rows >= 1
    @assert n_cols >= 1
    @assert max_rowsum >= 1

    X = zeros(Bool, (n_rows, n_cols))
    for i in (1:n_rows)[
            repeat([1, 2],convert(Int64,round(n_rows/2)))[1:n_rows].==1
            ]
        rowsum = rand(1:max_rowsum)
        fill = sample(1:n_cols, rowsum, replace = false)
        X[i, fill] .= true
        X[i+1, fill] .= true
    end
    X
end


function generate_weights_with_max_rowsum(X, max_row_weight, prop_capacity)
    n_rows, n_cols = size(X)
    w_min = 2*prop_capacity - 1
    total_weights = max_row_weight .* rand(Uniform(w_min, 1), n_rows)
    W = zeros(n_rows, n_cols)
    for i in 1:n_rows
        W[i, X[i,:]] = total_weights[i] * rand(Dirichlet(sum(X[i,:]), 1.0))
    end
    W
end

function add_random_disruption(S::Matrix{Bool})
    @assert sum(S) > 0
    nonzero_entries = findall(S)
    entry = rand(nonzero_entries)
    Sp = Array(S)
    Sp[entry] = 0
    Sp
end

### MODEL COMPUTATIONS ###

# function compute_phenotype(S, C)
#     @assert S == (C .!= 0)
#     sum(S .* C; dims = 1)[:] # Can't this just be sum(C, dims=1)[:]?
# end

function compute_phenotype(C)
    #@assert S == (C .!= 0)
    sum(C; dims = 1)[:] # Can't this just be sum(C, dims=1)[:]?
end

### OPTIMIZATION ###

"""
Return real-valued edge weights, as a `Vector{Float64}`, that optimize an
objective function analogous to the integer-valued one in Whitacre et al., given
a target phenotype and a set of edges.

The objective function is the sum of the differences between the target
phenotype and the realized phenotype, for effectors where the realized phenotype
value is less than the target value.

The total weight for each receptor (row) is fixed at
`total_weight_per_receptor`. This is equivalent to constraining it to be less
than `total_weight_per_receptor`, because excess weight in a row does not affect
the objective function.

If `weights` is the returned weight vector, `weights[i]` is the weight
corresponding to edge `edges[i]`.
"""
function optimize_weights_real_whitacre(
    target_phenotype::Vector{Float64},
    S,
    total_weight_per_receptor::Float64;
    n_initial_conditions::Int = 1,
    initial_range = 1.0,
    return_all = false
)
    edges = findall(S)
    n_receptors, n_effectors = size(S)

    # Helpful transformations of the graph for our various computations
    n_edges = length(edges)
    receptors = [edge[1] for edge in edges]
    effectors = [edge[2] for edge in edges]

    # Define function to compute weight from unconstrained parameters
    w_from_u = function(u)
        expu = exp.(u) #u has length of non-zero elements of S?
        total_expu = zeros(n_receptors)
        for (i, receptor) in enumerate(receptors)
            total_expu[receptor] += expu[i]
        end
        total_weight_per_receptor * expu ./ total_expu[receptors]
    end

    # Define objective function in terms of unconstrained parameters u,
    # transformed into
    # w[i] = total_weight_per_receptor * exp(u[i]) / sum(exp(u))
    # for edge i corresponding to (receptor_i, effector_i)
    # where the sum is across all edges with the same receptor as edge i
    obj_func = function(u)
        # Compute weights
        w = w_from_u(u)

        # Compute phenotype from weights
        phenotype = zeros(n_effectors)
        for (i, effector) in enumerate(effectors)
            phenotype[effector] += w[i]
        end

        # Sum of positive elements of (target_phenotype .- phenotype)
        sum(max.(0.0, target_phenotype .- phenotype))
    end

    # obj_func = function(u)
    #     # Compute weights
    #     w = w_from_u(u)
    #
    #     # Compute phenotype from weights
    #     phenotype = zeros(n_effectors)
    #     for (i, effector) in enumerate(effectors)
    #         phenotype[effector] += w[i]
    #     end
    #
    #     # product of elements of (target_phenotype .- phenotype)
    #     prod(max.(1.0, phenotype ./ target_phenotype))
    # end

    # Define function to generate initial conditions
    function make_initial_condition()
        initial_range * 2.0 * (rand(n_edges) .- 0.5)
    end

    # Repeat `n_initial_conditions` times and take the best one
    obj_min = Inf
    w_min = nothing

    obj_all = nothing
    w_all = nothing

    if return_all
        obj_all = []
        w_all = []
    end

    nm = NelderMead()
    for i in 1:n_initial_conditions
        # Run optimization using Nelder-Mead
        opt_result = optimize(obj_func, make_initial_condition(), nm)

        if return_all
            push!(obj_all, opt_result.minimum)
            push!(w_all, w_from_u(opt_result.minimizer))
        elseif opt_result.minimum < obj_min
            obj_min = opt_result.minimum
            w_min = w_from_u(opt_result.minimizer)
        end
    end

    if return_all
        obj_min, index_min = findmin(obj_all)
        w_min = w_all[index_min]
        C_min = vector_to_matrix_with_edges(
            n_receptors, n_effectors, w_min, edges
        )
        (C_min, obj_all, w_all)
    else
        (C_min, [obj_min], [w_min])
    end
end

"""

Notation for regression follows Bishop (2006),
*Pattern Recognition and Machine Learning*, p. 153.
"""
function optimize_weights_regression_analytical(
    target_phenotype::Vector{Float64},
    S,
    weight_prior_sd::Float64;
    apply_prior_per_receptor = true
)
    edges = findall(S)
    n_receptors, n_effectors = size(S)

    receptors = [edge[1] for edge in edges]
    effectors = [edge[2] for edge in edges]
    n_edges = length(edges)

    # Construct design matrix for linear regression:
    # Each row corresponds to an effector (note, effectors are columns in S)
    # and each column corresponds to an edge.
    # The predictors/data in the design matrix are:
    # does an edge contribute to an effector?
    # If it does, then there's an 1 in the matrix for that edge/effector
    # combination. Otherwise, there's a 0.
    Phi = zeros((n_effectors, n_edges))
    for (edge_index, effector) in enumerate(effectors)
        Phi[effector, edge_index] = 1.0
    end

    # Construct prior precision matrix.
    # If the prior is per receptor, then we set the total variance for all
    # edges associated with the receptor to be as provided.
    # Variance adds, so the variance per edge is divided by the number of edges.
    # I.e., the precision is multiplied by the number of edges.
    prec = 1.0 / weight_prior_sd^2
    w_prior_prec = Diagonal(
        if apply_prior_per_receptor
            # Higher precision/lower variance per entry for more entries
            prec * (sum(S; dims = 2)[receptors])
        else
            precision * ones(n_edges)
        end
    )
    w_prior_cov = inv(w_prior_prec)

    PhiT = transpose(Phi)

    # Construct posterior inverse covariance matrix
    w_post_prec = w_prior_prec + PhiT * Phi

    w_post_cov = inv(w_post_prec)
    w_post_mean = w_post_cov * (PhiT * target_phenotype)

    C = vector_to_matrix_with_edges(
        n_receptors, n_effectors, w_post_mean, edges
    )

    (C, w_post_mean, w_post_cov)
end


### ODE/FITNESS FUNCTION ###

"""
Return the solution to the viral dynamics ODE and the corresponding fitness
function(s), as a vector of times and a matrix with columns (V, I, F, G),
and final values of the fitness functions F and G.

dV/dt = µ V - k I V
dI/dt = β I V / (ϕ + V) - α I

* V : viral density
* I : density of innate immune effectors
* μ : maximal net viral growth rate
* k : killing power of immune effectors
* β : maximum growth rate of immunity
* ϕ : half-saturation constant
* α : decay rate of innate immunity

Includes two options for the fitness function:

* F(T), the integral of log(1 + V(t))
* G(T), the integral of V(t)
"""
function solve_host_virus(V0, I0, μ, k, β, ϕ, α, tspan)
    prob = ODEProblem(ddt_host_virus!, [V0; I0; 0; 0], tspan, [μ, k, β, ϕ, α])
    sol = solve(prob)

    t_vec = sol.t
    u_mat = nested_vector_to_matrix(sol.u)

    (t_vec, u_mat, u_mat[end,3], u_mat[end,4])
end

# function ddt_host_virus!(du, u, p, t)
#     V, I, F = u
#     μ, k, β, ϕ, α = p
#
#     dVdt = μ * V - k * I * V
#     dIdt = β * I * V / (ϕ + V) - α * I
#     if V > -1
#         dFdt = log(1 + V)
#     else
#         dFdt = 0
#     end
#     dGdt = V
#
#
#     du[1] = dVdt
#     du[2] = dIdt
#     du[3] = dFdt
#     du[4] = dGdt
# end


function hill(l, n, ka)
  1 / (1 + (ka/l)^n)
end

function ddt_host_virus!(du, u, p, t)
    V, I, F = u
    μ, k, β, ϕ, α = p

    dVdt = μ * V - k * I * V
    dIdt = β * I * V / (ϕ + V) - α * I
    # if V > -1
    #     dFdt = log(1 + V)
    # else
    #     dFdt = 0
    # end
    dFdt = hill(V, 10, 500)
    dGdt = V


    du[1] = dVdt
    du[2] = dIdt
    du[3] = dFdt
    du[4] = dGdt
end



### NEUTRAL NETWORK COMPUTATION ###

"""
    Compute the neutral network around an initial S0 / C0.
"""
function compute_neutral_network(
    S0, C0, neutrality_threshold,
    optimization_function, fitness_function;
    output_callback = nothing
)
    initial_fitness = fitness_function(S0, C0)

    if !ismissing(output_callback)
        output_callback(0, nothing, nothing, S0, C0, initial_fitness)
    end

    # S matrices in the neutral network:
    # dictionary mapping from matrix to fitness value
    neutral_network = Dict{BitMatrix, Float64}()

    # S matrices in the 1-neighborhood
    one_neighborhood = Dict{BitMatrix, Float64}()

    next_id = 1

    function compute_neutral_network_recursive(parent_id, S_parent)
        edges = findall(S_parent)
        if length(edges) == 0
            return nothing
        end

        for edge in edges
            S = remove_edge(S_parent, edge)
            id = next_id
            next_id += 1

            if haskey(neutral_network, S) || haskey(one_neighborhood, S)
#                 println("found repeat matrix")
                continue
            end

            C = optimization_function(S)
            fitness = fitness_function(S, C)

            if abs(fitness - initial_fitness) < neutrality_threshold
                @show fitness, initial_fitness
#                 println("found something neutral")
                neutral_network[S] = fitness
                compute_neutral_network_recursive(id, S)
            else
#                 println("found something non-neutral")
                one_neighborhood[S] = fitness
            end

            if !isnothing(output_callback)
                output_callback(id, parent_id, edge, S, C, fitness)
            end
        end

        nothing
    end

    compute_neutral_network_recursive(0, S0)
    (neutral_network, one_neighborhood)
end

function remove_edge(S, edge)
    Sp = BitMatrix(S)
    Sp[edge] = 0
    Sp
end


### FITNESS LANDSCAPE COMPUTATION ###

"""
    Compute the fitness landscape around an initial S0 / C0.
"""
function compute_fitness_landscape(
    S0, C0,
    optimization_function, fitness_function,
    min_edges;
    output_callback = nothing
)
    initial_fitness = fitness_function(S0, C0)

    if !ismissing(output_callback)
        output_callback(0, nothing, nothing, S0, C0, initial_fitness)
    end

    # S matrices
    # dictionary mapping from matrix to fitness value
    fitness_landscape = Dict{BitMatrix, Float64}()

    next_id = 1

    function compute_fitness_landscape_recursive(parent_id, S_parent)
        edges = findall(S_parent)
        if length(edges) <= min_edges
            return nothing
        end

        for edge in edges
            S = remove_edge(S_parent, edge)
            id = next_id
            next_id += 1

            if haskey(fitness_landscape, S)
#                 println("found repeat matrix")
                continue
            end

            C = optimization_function(S)
            fitness = fitness_function(S, C)

            fitness_landscape[S] = fitness
            if !isnothing(output_callback)
                output_callback(id, parent_id, edge, S, C, fitness)
            end

            compute_fitness_landscape_recursive(id, S)
        end

        nothing
    end
    compute_fitness_landscape_recursive(0, S0)
    (fitness_landscape)
end

function remove_edge(S, edge)
    Sp = BitMatrix(S)
    Sp[edge] = 0
    Sp
end
